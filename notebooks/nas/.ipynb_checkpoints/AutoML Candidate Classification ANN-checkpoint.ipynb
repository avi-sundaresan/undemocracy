{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1661881246964,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "egqMGfopyf0I",
    "outputId": "1a505289-b0d1-485d-c173-0242f1fd1abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout, LSTM\n",
    "import keras.utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.backend import log\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1661881273887,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "w4ldiR5W1pbP"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = 'undemocracy/notebooks/nas/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661886353241,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "ynYuY-BlkKd1"
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   NAS PARAMETERS                     #\n",
    "########################################################\n",
    "CONTROLLER_SAMPLING_EPOCHS = 10\n",
    "SAMPLES_PER_CONTROLLER_EPOCH = 10\n",
    "CONTROLLER_TRAINING_EPOCHS = 10\n",
    "ARCHITECTURE_TRAINING_EPOCHS = 10\n",
    "CONTROLLER_LOSS_ALPHA = 0.9\n",
    "REINFORCE_BASELINE = 0.8\n",
    "\n",
    "########################################################\n",
    "#               CONTROLLER PARAMETERS                  #\n",
    "########################################################\n",
    "CONTROLLER_LSTM_DIM = 100\n",
    "CONTROLLER_OPTIMIZER = 'Adam'\n",
    "CONTROLLER_LEARNING_RATE = 0.01\n",
    "CONTROLLER_DECAY = 0.1\n",
    "CONTROLLER_MOMENTUM = 0.0\n",
    "CONTROLLER_USE_PREDICTOR = False\n",
    "\n",
    "########################################################\n",
    "#                   MLP PARAMETERS                     #\n",
    "########################################################\n",
    "MAX_ARCHITECTURE_LENGTH = 3\n",
    "MLP_OPTIMIZER = 'Adam'\n",
    "MLP_LEARNING_RATE = 0.01\n",
    "MLP_DECAY = 0.0\n",
    "MLP_MOMENTUM = 0.0\n",
    "MLP_DROPOUT = 0.2\n",
    "MLP_LOSS_FUNCTION = 'binary_crossentropy'\n",
    "MLP_ONE_SHOT = True\n",
    "\n",
    "########################################################\n",
    "#                   DATA PARAMETERS                    #\n",
    "########################################################\n",
    "TARGET_CLASSES = 2\n",
    "\n",
    "########################################################\n",
    "#                  OUTPUT PARAMETERS                   #\n",
    "########################################################\n",
    "TOP_N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661886353473,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "BVxDummk3_8Q"
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = numpy.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF6k8kXgjVuX"
   },
   "source": [
    "## MLP Search Space Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1661886353778,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "BLOe0Sl5X5j4"
   },
   "outputs": [],
   "source": [
    "class MLPSearchSpace(object):\n",
    "\n",
    "    def __init__(self, target_classes):\n",
    "\n",
    "        self.target_classes = target_classes\n",
    "        self.vocab = self.vocab_dict()\n",
    "\n",
    "    def vocab_dict(self):\n",
    "    \t# define the allowed nodes and activation functions\n",
    "        nodes = [8, 12, 16, 24, 32, 48] #consider making these numbers larger\n",
    "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "        \n",
    "        # initialize lists for keys and values of the vocabulary\n",
    "        layer_params = []\n",
    "        layer_id = []\n",
    "        \n",
    "        # for all activation functions for each node\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(act_funcs)):\n",
    "            \t\n",
    "                # create an id and a configuration tuple (node, activation)\n",
    "                layer_params.append((nodes[i], act_funcs[j]))\n",
    "                layer_id.append(len(act_funcs) * i + j + 1)\n",
    "        \n",
    "        # zip the id and configurations into a dictionary\n",
    "        vocab = dict(zip(layer_id, layer_params))\n",
    "        \n",
    "        # add dropout in the volcabulary\n",
    "        vocab[len(vocab) + 1] = (('dropout'))\n",
    "        \n",
    "        # add the final softmax/sigmoid layer in the vocabulary\n",
    "        if self.target_classes == 2:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
    "        else:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
    "        return vocab\n",
    "\n",
    "\n",
    "\t# function to encode a sequence of configuration tuples\n",
    "    def encode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        encoded_sequence = []\n",
    "        for value in sequence:\n",
    "            encoded_sequence.append(keys[values.index(value)])\n",
    "        return encoded_sequence\n",
    "\n",
    "\n",
    "\t# function to decode a sequence back to configuration tuples\n",
    "    def decode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        decoded_sequence = []\n",
    "        for key in sequence:\n",
    "            decoded_sequence.append(values[keys.index(key)])\n",
    "        return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRR2z5BXjZRY"
   },
   "source": [
    "## MLP Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1661886354351,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "oKrFdunYdk6e"
   },
   "outputs": [],
   "source": [
    "class MLPGenerator(MLPSearchSpace):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.mlp_optimizer = MLP_OPTIMIZER\n",
    "        self.mlp_lr = MLP_LEARNING_RATE\n",
    "        self.mlp_decay = MLP_DECAY\n",
    "        self.mlp_momentum = MLP_MOMENTUM\n",
    "        self.mlp_dropout = MLP_DROPOUT\n",
    "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
    "        self.mlp_one_shot = MLP_ONE_SHOT\n",
    "        self.metrics = ['accuracy']\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        if self.mlp_one_shot:\n",
    "\t\n",
    "            # path to shared weights file \n",
    "            self.weights_file = BASE_PATH + 'LOGS/shared_weights.pkl'\n",
    "    \n",
    "            # open an empty dataframe with columns for bigrams IDs and weights\n",
    "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
    "        \n",
    "            # pickle the dataframe\n",
    "            if not os.path.exists(self.weights_file):\n",
    "                print(\"Initializing shared weights dictionary...\")\n",
    "                self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "\n",
    "    # function to create a keras model given a sequence and input data shape\n",
    "    def create_model(self, sequence, mlp_input_shape):\n",
    "\n",
    "        # decode sequence to get nodes and activations of each layer\n",
    "        layer_configs = self.decode_sequence(sequence)\n",
    "\n",
    "        # create a sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # add a flatten layer if the input is 3 or higher dimensional\n",
    "        if len(mlp_input_shape) > 1:\n",
    "            model.add(Flatten(name='flatten', input_shape=mlp_input_shape))\n",
    "\n",
    "            # for each element in the decoded sequence\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "\n",
    "                # add a model layer (Dense or Dropout)\n",
    "                if layer_conf is 'dropout':\n",
    "                    model.add(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))\n",
    "\n",
    "        else:\n",
    "            # for 2D inputs\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "\n",
    "                # add the first layer (requires the input shape parameter)\n",
    "                if i == 0:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1], input_shape=mlp_input_shape))\n",
    "\n",
    "                # add subsequent layers (Dense or Dropout)\n",
    "                elif layer_conf is 'dropout':\n",
    "                    model.add(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))\n",
    "\n",
    "        # return the keras model\n",
    "        return model\n",
    "\n",
    "    # function to compile the model with the appropriate optimizer and loss function\n",
    "    def compile_model(self, model):\n",
    "\n",
    "        # get optimizer\n",
    "        if self.mlp_optimizer == 'sgd':\n",
    "            optim = tf.optimizers.SGD(learning_rate=self.mlp_lr, decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
    "        elif self.mlp_optimizer == 'Adam':\n",
    "            optim = tf.optimizers.Adam(learning_rate=self.mlp_lr, decay=self.mlp_decay)\n",
    "        else:\n",
    "            optim = getattr(tf.optimizers, self.mlp_optimizer)(learning_rate=self.mlp_lr, decay=self.mlp_decay)\n",
    "\n",
    "        # compile model \n",
    "        model.compile(loss=self.mlp_loss_func, optimizer=optim, metrics=self.metrics)\n",
    "\n",
    "        # return the compiled keras model\n",
    "        return model\n",
    "\n",
    "    # --------------  ONESHOT STUFF --------------\n",
    "\n",
    "    def set_model_weights(self, model):\n",
    "    \n",
    "        # get nodes and activations for each layer    \n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            \n",
    "            # add flatten since it affects the size of the weights\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            \n",
    "            # don't add dropout since it doesn't affect weight sizes or activations\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        # get bigrams of relevant layers for weights transfer\n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        # for all layers\n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                \n",
    "                # get all bigram values we already have weights for\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                \n",
    "                # check if a bigram already exists in the dataframe\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                \n",
    "                # set layer weights if there is a bigram match in the dataframe \n",
    "                if len(search_index) > 0:\n",
    "                    print(\"Transferring weights for layer:\", config_ids[j])\n",
    "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
    "                j += 1\n",
    "        \n",
    "    def update_weights(self, model):\n",
    "\n",
    "        # get nodes and activations for each layer\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            \n",
    "            # add flatten since it affects the size of the weights\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            \n",
    "            # don't add dropout since it doesn't affect weight sizes or activations\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        # get bigrams of relevant layers for weights transfer\n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        # for all layers\n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                \n",
    "                #get all bigram values we already have weights for\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                \n",
    "                # check if a bigram already exists in the dataframe\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                \n",
    "                # add weights to df in a new row if weights aren't already available\n",
    "                if len(search_index) == 0:\n",
    "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j],\n",
    "                                                                    'weights': layer.get_weights()},\n",
    "                                                                    ignore_index=True)\n",
    "                # else update weights \n",
    "                else:\n",
    "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
    "                j += 1\n",
    "        self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def train_model(self, model, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
    "        if self.mlp_one_shot:\n",
    "            self.set_model_weights(model)\n",
    "            history = model.fit(x_data,\n",
    "                                y_data,\n",
    "                                epochs=nb_epochs,\n",
    "                                validation_split=validation_split,\n",
    "                                callbacks=callbacks,\n",
    "                                verbose=0)\n",
    "            self.update_weights(model)\n",
    "        else:\n",
    "            history = model.fit(x_data,\n",
    "                                y_data,\n",
    "                                epochs=nb_epochs,\n",
    "                                validation_split=validation_split,\n",
    "                                callbacks=callbacks,\n",
    "                                verbose=0)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9hpN6tkjc7D"
   },
   "source": [
    "## LSTM RNN Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1661886354559,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "3hwIVBFLb7i0"
   },
   "outputs": [],
   "source": [
    "class Controller(MLPSearchSpace):\n",
    "\n",
    "    def __init__(self):\n",
    "\t\t\n",
    "        # defining training and sequence creation related parameters\n",
    "        self.max_len = MAX_ARCHITECTURE_LENGTH\n",
    "        self.controller_lstm_dim = CONTROLLER_LSTM_DIM\n",
    "        self.controller_optimizer = CONTROLLER_OPTIMIZER\n",
    "        self.controller_lr = CONTROLLER_LEARNING_RATE\n",
    "        self.controller_decay = CONTROLLER_DECAY\n",
    "        self.controller_momentum = CONTROLLER_MOMENTUM\n",
    "        self.use_predictor = CONTROLLER_USE_PREDICTOR\n",
    "        \n",
    "        # file path of controller weights to be stored at\n",
    "        self.controller_weights = BASE_PATH + 'LOGS/controller_weights.h5'\n",
    "\n",
    "        # initializing a list for all the sequences created\n",
    "        self.seq_data = []\n",
    "\n",
    "        # inheriting from the search space\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        # number of classes for the controller (+ 1 for padding)\n",
    "        self.controller_classes = len(self.vocab) + 1\n",
    "\n",
    "    def control_model(self, controller_input_shape, controller_batch_size):\n",
    "        #main_input = Input(shape=controller_input_shape, batch_shape=controller_batch_size, name='main_input')\n",
    "        main_input = Input(shape=controller_input_shape,name='main_input')\n",
    "        x = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
    "        main_output = Dense(self.controller_classes, activation='softmax', name='main_output')(x)\n",
    "        model = Model(inputs=[main_input], outputs=[main_output])\n",
    "        return model\n",
    "\n",
    "    # HYBRID LSTM CONTROLLER: \n",
    "    #  - main output is the encoded sequence to be passed to the MPL Generator\n",
    "    #  - accuracy predictor output adds adversarial element to LSTM \n",
    "   \n",
    "    def hybrid_control_model(self, controller_input_shape, controller_batch_size):\n",
    "        # input layer initialized with input shape and batch size\n",
    "        main_input = Input(shape=controller_input_shape, batch_shape=controller_batch_size, name='main_input')\n",
    "        \n",
    "        # LSTM layer\n",
    "        x1 = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
    "        # output for the sequence generator network\n",
    "        main_output = Dense(self.controller_classes, activation='softmax', name='main_output')(x1)\n",
    "\n",
    "        # LSTM layer\n",
    "        x2 = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
    "        # single neuron sigmoid layer for accuracy prediction\n",
    "        predictor_output = Dense(1, activation='sigmoid', name='predictor_output')(x2)\n",
    "        \n",
    "        # finally the Keras Model class is used to create a multi-output model\n",
    "        model = Model(inputs=[main_input], outputs=[main_output, predictor_output])\n",
    "        return model\n",
    "    \n",
    "    def train_control_model(self, model, x_data, y_data, loss_func, controller_batch_size, nb_epochs):\n",
    "        # get the optimizer required for training\n",
    "        if self.controller_optimizer == 'sgd':\n",
    "            optim = tf.optimizers.SGD(learning_rate=self.controller_lr,\n",
    "                                decay=self.controller_decay,\n",
    "                                momentum=self.controller_momentum)\n",
    "        elif self.controller_optimizer == 'Adam':\n",
    "            optim = tf.optimizers.Adam(learning_rate=self.controller_lr,\n",
    "                                decay=self.controller_decay)\n",
    "        else:\n",
    "            optim = getattr(tf.optimizers, self.controller_optimizer)(learning_rate=self.controller_lr, \n",
    "                                                    decay=self.controller_decay)\n",
    "                                                    \n",
    "        # compile model depending on loss function and optimizer provided\n",
    "        model.compile(optimizer=optim, loss={'main_output': loss_func})\n",
    "        \n",
    "        # load controller weights\n",
    "        if os.path.exists(self.controller_weights):\n",
    "            model.load_weights(self.controller_weights)\n",
    "            \n",
    "        # train the controller\n",
    "        print(\"TRAINING CONTROLLER...\")\n",
    "        model.fit({'main_input': x_data},\n",
    "                {'main_output': y_data.reshape(len(y_data), 1, self.controller_classes)},\n",
    "                epochs=nb_epochs,\n",
    "                batch_size=controller_batch_size,\n",
    "                verbose=0)\n",
    "        \n",
    "        # save controller weights\n",
    "        model.save_weights(self.controller_weights)\n",
    "    \n",
    "    def train_hybrid_model(self, model, x_data, y_data, loss_func, controller_batch_size, nb_epochs):\n",
    "        print('Hybrid model not supported.')\n",
    "        # # get the optimizer required for training\n",
    "        # if self.controller_optimizer == 'sgd':\n",
    "        #     optim = optimizers.SGD(learning_rate=self.controller_lr,\n",
    "        #                         decay=self.controller_decay,\n",
    "        #                         momentum=self.controller_momentum)\n",
    "        # else:\n",
    "        #     optim = getattr(optimizers, self.controller_optimizer)(learning_rate=self.controller_lr, \n",
    "        #                                             decay=self.controller_decay)\n",
    "                                                    \n",
    "        # # compile model depending on loss function and optimizer provided\n",
    "        # model.compile(optimizer=optim,\n",
    "        #             loss={'main_output': loss_func, 'predictor_output': 'mse'},\n",
    "        #             loss_weights={'main_output': 1, 'predictor_output': 1})\n",
    "\n",
    "        # # load controller weights\n",
    "        # if os.path.exists(self.controller_weights):\n",
    "        #     model.load_weights(self.controller_weights)\n",
    "            \n",
    "        # # train the controller\n",
    "        # print(\"TRAINING CONTROLLER...\")\n",
    "        # model.fit({'main_input': x_data},\n",
    "        #         {'main_output': y_data.reshape(len(y_data), 1, self.controller_classes),\n",
    "        #         'predictor_output': np.array(pred_target).reshape(len(pred_target), 1, 1)},\n",
    "        #         epochs=nb_epochs,\n",
    "        #         batch_size=controller_batch_size,\n",
    "        #         verbose=0)\n",
    "        \n",
    "        # # save controller weights\n",
    "        # model.save_weights(self.controller_weights)\n",
    "    \n",
    "    def sample_architecture_sequences(self, model, number_of_samples):\n",
    "        # define values needed for sampling \n",
    "        final_layer_id = len(self.vocab)\n",
    "        dropout_id = final_layer_id - 1\n",
    "        vocab_idx = [0] + list(self.vocab.keys())\n",
    "        \n",
    "        # initialize list for architecture samples\n",
    "        samples = []\n",
    "        print(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
    "        print('------------------------------------------------------')\n",
    "        \n",
    "        # while number of architectures sampled is less than required\n",
    "        while len(samples) < number_of_samples:\n",
    "            \n",
    "            # initialise the empty list for architecture sequence\n",
    "            seed = []\n",
    "            \n",
    "            # while len of generated sequence is less than maximum architecture length\n",
    "            while len(seed) < self.max_len:\n",
    "                \n",
    "                # pad sequence for correctly shaped input for controller\n",
    "                sequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
    "                sequence = sequence.reshape(1, 1, self.max_len - 1)\n",
    "                \n",
    "                # given the previous elements, get softmax distribution for the next element\n",
    "                if self.use_predictor:\n",
    "                    (probab, _) = model.predict(sequence)\n",
    "                else:\n",
    "                    probab = model.predict(sequence)\n",
    "                probab = probab[0][0]\n",
    "                \n",
    "                # sample the next element randomly given the probability of next elements (the softmax distribution)\n",
    "                next = np.random.choice(vocab_idx, size=1, p=probab)[0]\n",
    "                \n",
    "                # first layer isn't dropout\n",
    "                if next == dropout_id and len(seed) == 0:\n",
    "                    continue\n",
    "                # first layer is not final layer\n",
    "                if next == final_layer_id and len(seed) == 0:\n",
    "                    continue\n",
    "                # if final layer, break out of inner loop\n",
    "                if next == final_layer_id:\n",
    "                    seed.append(next)\n",
    "                    break\n",
    "                # if sequence length is 1 less than maximum, add final\n",
    "                # layer and break out of inner loop\n",
    "                if len(seed) == self.max_len - 1:\n",
    "                    seed.append(final_layer_id)\n",
    "                    break\n",
    "                # ignore padding\n",
    "                if not next == 0:\n",
    "                    seed.append(next)\n",
    "            \n",
    "            # check if the generated sequence has been generated before.\n",
    "            # if not, add it to the sequence data. \n",
    "            if seed not in self.seq_data:\n",
    "                samples.append(seed)\n",
    "                self.seq_data.append(seed)\n",
    "        return samples\n",
    "    \n",
    "    def get_predicted_accuracies_hybrid_model(self, model, seqs):\n",
    "        pred_accuracies = []        \n",
    "        for seq in seqs:\n",
    "            # pad each sequence\n",
    "            control_sequences = pad_sequences([seq], maxlen=self.max_len, padding='post')\n",
    "            xc = control_sequences[:, :-1].reshape(len(control_sequences), 1, self.max_len - 1)\n",
    "            # get predicted accuracies\n",
    "            (_, pred_accuracy) = [x[0][0] for x in model.predict(xc)]\n",
    "            pred_accuracies.append(pred_accuracy[0])\n",
    "        return pred_accuracies    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jj9O88DkjhSO"
   },
   "source": [
    "## Combined MLPNAS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1661886354927,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "lwqsjSdlaQ9R"
   },
   "outputs": [],
   "source": [
    "class MLPNAS(Controller):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.controller_sampling_epochs = CONTROLLER_SAMPLING_EPOCHS\n",
    "        self.samples_per_controller_epoch = SAMPLES_PER_CONTROLLER_EPOCH\n",
    "        self.controller_train_epochs = CONTROLLER_TRAINING_EPOCHS\n",
    "        self.architecture_train_epochs = ARCHITECTURE_TRAINING_EPOCHS\n",
    "        self.controller_loss_alpha = CONTROLLER_LOSS_ALPHA\n",
    "\n",
    "        self.data = []\n",
    "        self.nas_data_log = BASE_PATH + 'LOGS/nas_data.pkl'\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_generator = MLPGenerator()\n",
    "\n",
    "        self.controller_batch_size = len(self.data)\n",
    "        self.controller_input_shape = (1, MAX_ARCHITECTURE_LENGTH - 1)\n",
    "        if self.use_predictor:\n",
    "            self.controller_model = self.hybrid_control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "        else:\n",
    "            self.controller_model = self.control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "\n",
    "    \n",
    "    # create architectures using encoded sequences we got from the controller\n",
    "    def create_architecture(self, sequence):\n",
    "        # create the model using the model generator\n",
    "        model = self.model_generator.create_model(sequence, np.shape(self.x[0]))\n",
    "        \n",
    "        # compile said model\n",
    "        model = self.model_generator.compile_model(model)\n",
    "        return model\n",
    "\n",
    "    # train the generated architecture\n",
    "    def train_architecture(self, model):\n",
    "        \n",
    "        # shuffle the x and y data\n",
    "        x, y = unison_shuffled_copies(self.x, self.y)\n",
    "        \n",
    "        # train the model\n",
    "        history = self.model_generator.train_model(model, x, y, self.architecture_train_epochs)\n",
    "        return history\n",
    "    \n",
    "    def append_model_metrics(self, sequence, history, pred_accuracy=None):\n",
    "\t\t\n",
    "        # if the MLP models are trained only for a single epoch\n",
    "        if len(history.history['val_accuracy']) == 1:\n",
    "        \t\n",
    "            # if an accuracy predictor is used\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                  history.history['val_accuracy'][0],\n",
    "                                  pred_accuracy])\n",
    "            \n",
    "            # if no accuracy predictor data available\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                  history.history['val_accuracy'][0]])\n",
    "            print('validation accuracy: ', history.history['val_accuracy'][0])\n",
    "        \n",
    "        # if the MLP models are trained for more than one epoch\n",
    "        else:\n",
    "        \t\n",
    "            # take a moving average of validation accuracy across epochs\n",
    "            val_acc = np.ma.average(history.history['val_accuracy'],\n",
    "                                    weights=np.arange(1, len(history.history['val_accuracy']) + 1),\n",
    "                                    axis=-1)\n",
    "            \n",
    "            # add predicted accuracies if available else don't\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                  val_acc,\n",
    "                                  pred_accuracy])\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                  val_acc])\n",
    "            print('validation accuracy: ', val_acc)\n",
    "    \n",
    "\n",
    "    def prepare_controller_data(self, sequences):\n",
    "        \n",
    "        # pad generated sequences to maximum length\n",
    "        controller_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        # split into inputs and labels for LSTM controller\n",
    "        xc = controller_sequences[:, :-1].reshape(len(controller_sequences), 1, self.max_len - 1)\n",
    "        yc = to_categorical(controller_sequences[:, -1], self.controller_classes)\n",
    "        \n",
    "        # get validation accuracies for each for reward function\n",
    "        val_acc_target = [item[1] for item in self.data]\n",
    "        return xc, yc, val_acc_target\n",
    "\n",
    "    def get_discounted_reward(self, rewards):\n",
    "        \n",
    "        # initialise discounted reward array\n",
    "        discounted_r = np.zeros_like(rewards, dtype=np.float32)\n",
    "        \n",
    "        # every element in the discounted reward array\n",
    "        for t in range(len(rewards)):\n",
    "            running_add = 0.\n",
    "            exp = 0.\n",
    "            \n",
    "            # will need us to iterate over all rewards from t to T\n",
    "            for r in rewards[t:]:\n",
    "                running_add += self.controller_loss_alpha**exp * r\n",
    "                exp += 1\n",
    "            \n",
    "            # add values to the discounted reward array\n",
    "            discounted_r[t] = running_add\n",
    "        \n",
    "        # normalize discounted reward array\n",
    "        discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
    "        return discounted_r\n",
    "\n",
    "\t# loss function based on discounted reward for policy gradients\n",
    "    def custom_loss(self, target, output):\n",
    "        \n",
    "        # define baseline for rewards and subtract it from all validation accuracies to get reward. \n",
    "        baseline = REINFORCE_BASELINE\n",
    "        reward = np.array([item[1] - baseline for item in self.data[-self.samples_per_controller_epoch:]]).reshape(\n",
    "            self.samples_per_controller_epoch, 1)\n",
    "        \n",
    "        # get discounted reward\n",
    "        discounted_reward = self.get_discounted_reward(reward)\n",
    "        \n",
    "        # multiply discounted reward by log likelihood of actions to get loss function\n",
    "        loss = - log(output) * discounted_reward[:, None]\n",
    "        return loss\n",
    "    \n",
    "    # calls controller training function from above\n",
    "    def train_controller(self, model, x, y, pred_accuracy=None):\n",
    "        if self.use_predictor:\n",
    "            self.train_hybrid_model(model,\n",
    "                                    x,\n",
    "                                    y,\n",
    "                                    pred_accuracy,\n",
    "                                    self.custom_loss,\n",
    "                                    len(self.data),\n",
    "                                    self.controller_train_epochs)\n",
    "        else:\n",
    "            self.train_control_model(model,\n",
    "                                     x,\n",
    "                                     y,\n",
    "                                     self.custom_loss,\n",
    "                                     len(self.data),\n",
    "                                     self.controller_train_epochs)\n",
    "    \n",
    "    # Main NAS Loop \n",
    "\n",
    "    def search(self):\n",
    "    \n",
    "        # for every controller epoch\n",
    "        for controller_epoch in range(self.controller_sampling_epochs):\n",
    "            \n",
    "            # generate sequences\n",
    "            sequences = self.sample_architecture_sequences(self.controller_model, self.samples_per_controller_epoch)\n",
    "            \n",
    "            # if using a predictor, predict their accuracies\n",
    "            if self.use_predictor:\n",
    "                pred_accuracies = self.get_predicted_accuracies_hybrid_model(self.controller_model, sequences)\n",
    "            \n",
    "            # for each sequence generated in a controller epoch\n",
    "            for i, sequence in enumerate(sequences):\n",
    "                \n",
    "                # create an MLP model \n",
    "                model = self.create_architecture(sequence)\n",
    "                \n",
    "                # train said MLP model\n",
    "                history = self.train_architecture(model)\n",
    "                \n",
    "                # log the model metrics\n",
    "                if self.use_predictor:\n",
    "                    self.append_model_metrics(sequence, history, pred_accuracies[i])\n",
    "                else:\n",
    "                    self.append_model_metrics(sequence, history)\n",
    "                            \n",
    "            # prepare data for the controller\n",
    "            xc, yc, val_acc_target = self.prepare_controller_data(sequences)\n",
    "            \n",
    "            # train the controller\n",
    "            self.train_controller(self.controller_model,\n",
    "                                  xc,\n",
    "                                  yc,\n",
    "                                  val_acc_target[-self.samples_per_controller_epoch:])\n",
    "        \n",
    "        # save all the NAS logs in a pickle file\n",
    "        with open(self.nas_data_log, 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "        \n",
    "        return self.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1661887004813,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "3f15jM-Y4lSt"
   },
   "outputs": [],
   "source": [
    "# NAS Eval Code\n",
    "    \n",
    "# def get_latest_event_id():\n",
    "#     all_subdirs = [BASE_PATH + 'LOGS/' + d for d in os.listdir('LOGS') if os.path.isdir('LOGS/' + d)]\n",
    "#     latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
    "#     return int(latest_subdir.replace('LOGS/event', ''))\n",
    "\n",
    "def load_nas_data():\n",
    "    # event = get_latest_event_id()\n",
    "    data_file = BASE_PATH + 'LOGS/nas_data.pkl'\n",
    "    with open(data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sort_search_data(nas_data):\n",
    "    val_accs = [item[1] for item in nas_data]\n",
    "    sorted_idx = np.argsort(val_accs)[::-1]\n",
    "    nas_data = [nas_data[x] for x in sorted_idx]\n",
    "    return nas_data\n",
    "\n",
    "def get_top_n_architectures(n):\n",
    "    data = load_nas_data()\n",
    "    data = sort_search_data(data)\n",
    "    search_space = MLPSearchSpace(TARGET_CLASSES)\n",
    "    print('Top {} Architectures:'.format(n))\n",
    "    for seq_data in data[:n]:\n",
    "        print('Architecture', search_space.decode_sequence(seq_data[0]))\n",
    "        print('Validation Accuracy:', seq_data[1])\n",
    "\n",
    "def get_nas_accuracy_plot():\n",
    "    data = load_nas_data()\n",
    "    accuracies = [x[1] for x in data]\n",
    "    plt.plot(np.arange(len(data)), accuracies)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_accuracy_distribution():\n",
    "    event = get_latest_event_id()\n",
    "    data = load_nas_data()\n",
    "    accuracies = [x[1]*100. for x in data]\n",
    "    accuracies = [int(x) for x in accuracies]\n",
    "    sorted_accs = np.sort(accuracies)\n",
    "    count_dict = {k: len(list(v)) for k, v in groupby(sorted_accs)}\n",
    "    plt.bar(list(count_dict.keys()), list(count_dict.values()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1661886355102,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "lYaICFpgr_kG"
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(BASE_PATH + 'new_data.csv')\n",
    "dataset = new_data[['Median Age', 'White', 'Black', 'AI/AN', 'PI', 'Other', 'Two Plus', 'Emp-LF Ratio', 'High School', 'Bachelor', 'Advanced', '% households w/ seniors', '% poverty', 'party_democrat', 'party_republican', 'inc_democrat', 'inc_republican', 'winner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661886355102,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "lBU8uMjeaUdv"
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:16].values\n",
    "Y = dataset.iloc[:,17].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220172,
     "status": "ok",
     "timestamp": 1661886575431,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "eyOp87ky7T-R",
    "outputId": "809e74e0-3246-4fd7-9dac-16d60b7b7e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "validation accuracy:  0.7911344929174944\n",
      "validation accuracy:  0.912847492911599\n",
      "validation accuracy:  0.9206611470742659\n",
      "validation accuracy:  0.9298272013664246\n",
      "validation accuracy:  0.9304282578555021\n",
      "validation accuracy:  0.9143501043319702\n",
      "validation accuracy:  0.9250187982212413\n",
      "Transferring weights for layer: ('input', (32, 'tanh'))\n",
      "validation accuracy:  0.9035311915657737\n",
      "validation accuracy:  0.9087903954766013\n",
      "Transferring weights for layer: ((32, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8879038409753279\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (12, 'elu'))\n",
      "validation accuracy:  0.8949662035161798\n",
      "validation accuracy:  0.8802404241128401\n",
      "Transferring weights for layer: ((12, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9056348638101057\n",
      "Transferring weights for layer: ((16, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8453794273463162\n",
      "validation accuracy:  0.8042073574933138\n",
      "Transferring weights for layer: ('input', (48, 'relu'))\n",
      "validation accuracy:  0.9026296236298301\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "validation accuracy:  0.9317806254733693\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "Transferring weights for layer: ((48, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9107438184998252\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((32, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9161532640457153\n",
      "Transferring weights for layer: ('input', (32, 'tanh'))\n",
      "validation accuracy:  0.8970698725093494\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (48, 'elu'))\n",
      "validation accuracy:  0.9006762038577687\n",
      "validation accuracy:  0.9075882824984464\n",
      "Transferring weights for layer: ((8, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9148008942604064\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.905334344777194\n",
      "Transferring weights for layer: ((48, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9371900861913508\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "validation accuracy:  0.8833959546956149\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "Transferring weights for layer: ((8, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8966190814971924\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8764838446270335\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "validation accuracy:  0.923365901816975\n",
      "Transferring weights for layer: ('input', (12, 'relu'))\n",
      "Transferring weights for layer: ((32, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9209616823629899\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((48, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8877535895867781\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((8, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9051840760491111\n",
      "Transferring weights for layer: ('input', (12, 'sigmoid'))\n",
      "Transferring weights for layer: ((48, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9465063853697343\n",
      "Transferring weights for layer: ('input', (12, 'tanh'))\n",
      "Transferring weights for layer: ((12, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.882344126701355\n",
      "Transferring weights for layer: ((12, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.817430515722795\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9015777772123164\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9122464375062422\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((12, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9060856699943542\n",
      "Transferring weights for layer: ('input', (8, 'tanh'))\n",
      "Transferring weights for layer: ((8, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.883095424825495\n",
      "Transferring weights for layer: ((8, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8293012684041804\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ((12, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9211119500073519\n",
      "Transferring weights for layer: ('input', (8, 'relu'))\n",
      "Transferring weights for layer: ((48, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.896168298071081\n",
      "Transferring weights for layer: ('input', (12, 'tanh'))\n",
      "validation accuracy:  0.8991735512560064\n",
      "Transferring weights for layer: ('input', (32, 'tanh'))\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9271224737167358\n",
      "Transferring weights for layer: ('input', (24, 'relu'))\n",
      "Transferring weights for layer: ((24, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9229151010513306\n",
      "Transferring weights for layer: ('input', (48, 'elu'))\n",
      "Transferring weights for layer: ((32, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9041322469711304\n",
      "Transferring weights for layer: ('input', (48, 'sigmoid'))\n",
      "Transferring weights for layer: ((16, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8924117218364369\n",
      "Transferring weights for layer: ('input', (24, 'sigmoid'))\n",
      "Transferring weights for layer: ((12, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9244177341461182\n",
      "Transferring weights for layer: ('input', (8, 'sigmoid'))\n",
      "Transferring weights for layer: ((8, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.952366647937081\n",
      "Transferring weights for layer: ('input', (12, 'sigmoid'))\n",
      "validation accuracy:  0.8871525255116549\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "validation accuracy:  0.949661910533905\n",
      "Transferring weights for layer: ('input', (12, 'sigmoid'))\n",
      "Transferring weights for layer: ((16, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9143501238389449\n",
      "Transferring weights for layer: ('input', (8, 'elu'))\n",
      "Transferring weights for layer: ((16, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.914350111918016\n",
      "Transferring weights for layer: ('input', (48, 'relu'))\n",
      "validation accuracy:  0.9123966943133961\n",
      "Transferring weights for layer: ('input', (24, 'elu'))\n",
      "Transferring weights for layer: ((16, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8996243574402549\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((48, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8888054284182462\n",
      "Transferring weights for layer: ('input', (12, 'relu'))\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9502629670229825\n",
      "Transferring weights for layer: ('input', (32, 'relu'))\n",
      "Transferring weights for layer: ((32, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9068369811231439\n",
      "Transferring weights for layer: ('input', (12, 'sigmoid'))\n",
      "Transferring weights for layer: ((32, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8513899239626798\n",
      "Transferring weights for layer: ('input', (12, 'relu'))\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9107437979091297\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (12, 'sigmoid'))\n",
      "Transferring weights for layer: ((24, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9636363571340387\n",
      "Transferring weights for layer: ('input', (48, 'relu'))\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8659654357216575\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((12, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9104432972994718\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "Transferring weights for layer: ((12, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9223140478134155\n",
      "Transferring weights for layer: ('input', (8, 'relu'))\n",
      "Transferring weights for layer: ((48, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9185574780810963\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "Transferring weights for layer: ((8, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8937640981240706\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((48, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9459053364667025\n",
      "Transferring weights for layer: ('input', (24, 'sigmoid'))\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8851991165768016\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9148009137673812\n",
      "Transferring weights for layer: ('input', (48, 'relu'))\n",
      "Transferring weights for layer: ((24, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9493613839149475\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (12, 'relu'))\n",
      "Transferring weights for layer: ((48, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9078888253732161\n",
      "Transferring weights for layer: ('input', (8, 'elu'))\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8883546330712059\n",
      "Transferring weights for layer: ('input', (12, 'tanh'))\n",
      "Transferring weights for layer: ((8, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.972351610660553\n",
      "Transferring weights for layer: ('input', (24, 'sigmoid'))\n",
      "Transferring weights for layer: ((24, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8960180390964855\n",
      "Transferring weights for layer: ('input', (8, 'sigmoid'))\n",
      "Transferring weights for layer: ((48, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8895567352121526\n",
      "Transferring weights for layer: ('input', (24, 'elu'))\n",
      "Transferring weights for layer: ((32, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8969196232882413\n",
      "Transferring weights for layer: ('input', (8, 'sigmoid'))\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8821938428011807\n",
      "Transferring weights for layer: ('input', (48, 'relu'))\n",
      "Transferring weights for layer: ((12, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9008264563300393\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "Transferring weights for layer: ((16, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8850488435138356\n",
      "Transferring weights for layer: ('input', (24, 'sigmoid'))\n",
      "Transferring weights for layer: ((48, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9347858894955028\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (12, 'relu'))\n",
      "Transferring weights for layer: ((12, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9081893422386863\n",
      "Transferring weights for layer: ((24, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9253193075006658\n",
      "Transferring weights for layer: ('input', (24, 'elu'))\n",
      "Transferring weights for layer: ((16, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9104432777924971\n",
      "Transferring weights for layer: ('input', (24, 'sigmoid'))\n",
      "Transferring weights for layer: ((12, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9603305697441101\n",
      "Transferring weights for layer: ('input', (12, 'elu'))\n",
      "Transferring weights for layer: ((24, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8679188565774397\n",
      "Transferring weights for layer: ((16, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9086401332508434\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "Transferring weights for layer: ((32, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8763335758989508\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8399699503725225\n",
      "Transferring weights for layer: ('input', (48, 'tanh'))\n",
      "Transferring weights for layer: ((24, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.904883552681316\n",
      "Transferring weights for layer: ('input', (12, 'tanh'))\n",
      "Transferring weights for layer: ((24, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9018782994963906\n",
      "TRAINING CONTROLLER...\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Transferring weights for layer: ('input', (32, 'tanh'))\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9015777707099915\n",
      "Transferring weights for layer: ('input', (8, 'relu'))\n",
      "Transferring weights for layer: ((24, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.8826446283947338\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "Transferring weights for layer: ((16, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9101427598433061\n",
      "Transferring weights for layer: ('input', (32, 'relu'))\n",
      "Transferring weights for layer: ((12, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9445529612627896\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "Transferring weights for layer: ((24, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.914650640704415\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((12, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9107438098300588\n",
      "Transferring weights for layer: ('input', (48, 'elu'))\n",
      "Transferring weights for layer: ((24, 'relu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9419984828342091\n",
      "Transferring weights for layer: ('input', (48, 'elu'))\n",
      "Transferring weights for layer: ((48, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9114951220425692\n",
      "Transferring weights for layer: ('input', (48, 'tanh'))\n",
      "Transferring weights for layer: ((48, 'tanh'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9188580047000539\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((32, 'elu'), (1, 'sigmoid'))\n",
      "validation accuracy:  0.9541698076508262\n",
      "TRAINING CONTROLLER...\n"
     ]
    }
   ],
   "source": [
    "nas_object = MLPNAS(X, Y)\n",
    "data = nas_object.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1661887009284,
     "user": {
      "displayName": "Avi Sundaresan",
      "userId": "08297856679941143696"
     },
     "user_tz": 420
    },
    "id": "vgjwxWUZD5Dy",
    "outputId": "54605c72-0135-42c8-a435-57a720752c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Architectures:\n",
      "Architecture [(12, 'tanh'), (8, 'relu'), (1, 'sigmoid')]\n",
      "Validation Accuracy: 0.972351610660553\n",
      "Architecture [(12, 'sigmoid'), (24, 'relu'), (1, 'sigmoid')]\n",
      "Validation Accuracy: 0.9636363571340387\n",
      "Architecture [(24, 'sigmoid'), (12, 'tanh'), (1, 'sigmoid')]\n",
      "Validation Accuracy: 0.9603305697441101\n",
      "Architecture [(32, 'elu'), (32, 'elu'), (1, 'sigmoid')]\n",
      "Validation Accuracy: 0.9541698076508262\n",
      "Architecture [(8, 'sigmoid'), (8, 'elu'), (1, 'sigmoid')]\n",
      "Validation Accuracy: 0.952366647937081\n"
     ]
    }
   ],
   "source": [
    "get_top_n_architectures(TOP_N)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AutoML Candidate Classification ANN.ipynb",
   "provenance": [
    {
     "file_id": "1h5MY5FM2tt50cFPsiBzs3arQ0AUpkiIN",
     "timestamp": 1661755241951
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
